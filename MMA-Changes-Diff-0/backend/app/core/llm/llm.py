import re
import json
import asyncio
import random
from app.utils.common_utils import transform_link, split_footnotes
from app.utils.log_util import logger
from app.schemas.response import (
    CoderMessage,
    WriterMessage,
    ModelerMessage,
    SystemMessage,
    CoordinatorMessage,
)
from app.services.redis_manager import redis_manager
from litellm import acompletion, token_counter
import litellm
from app.schemas.enums import AgentType
from app.utils.track import agent_metrics
from icecream import ic

# ====== å…¨å±€ï¼šè¯·æ±‚ä¸é‡è¯•é…ç½®ï¼ˆå¯æŒ‰éœ€è°ƒå¤§/è°ƒå°ï¼‰======
REQUEST_TIMEOUT = 300.0  # å•æ¬¡è¯·æ±‚æ•´ä½“è¶…æ—¶ï¼ˆç§’ï¼‰
HTTPX_TIMEOUTS = {
    "connect": 300.0,
    "read": 120.0,
    "write": 60.0,
    "pool": 120.0,
}
DEFAULT_MAX_RETRIES = 8
BACKOFF_BASE = 0.8  # æŒ‡æ•°é€€é¿åŸºæ•°ï¼Œå®é™… backoff = base * (2**attempt) + jitter

# ====== ä¸Šä¸‹æ–‡é•¿åº¦ä¿æŠ¤ï¼ˆç»™ DeepSeek/GPT ç­‰ç•™ä½™é‡ï¼‰======
# æ¨¡å‹æ ‡ç§°æœ€å¤§ 131072ï¼Œè¿™é‡Œä¿å®ˆé™åˆ¶åœ¨ 120000 å·¦å³ï¼Œé¿å…è§¦å‘ 400
CONTEXT_TOKEN_HARD_LIMIT = 120_000

litellm.callbacks = [agent_metrics]


class LLM:
    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str,
        task_id: str,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.chat_count = 0
        self.max_tokens: int | None = None  # æ·»åŠ æœ€å¤§tokenæ•°é™åˆ¶ï¼ˆç”¨äºé™åˆ¶è¾“å‡ºtokensï¼‰
        self.task_id = task_id

    async def chat(
        self,
        history: list = None,
        tools: list = None,
        tool_choice: str = None,
        max_retries: int = DEFAULT_MAX_RETRIES,  # æ·»åŠ æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: float = BACKOFF_BASE,  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿åŸºæ•°ï¼‰
        top_p: float | None = None,  # æ·»åŠ top_på‚æ•°
        agent_name: AgentType | str = AgentType.SYSTEM,  # â† æ”¾å®½ä¸ºæšä¸¾æˆ–å­—ç¬¦ä¸²
        sub_title: str | None = None,
    ) -> str:
        logger.info(f"subtitleæ˜¯:{sub_title}")

        # 1) éªŒè¯ & ä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§
        if history:
            history = self._validate_and_fix_tool_calls(history)

        # 2) æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆæŒ‰ token é™åˆ¶ï¼Œä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€è¿‘å¯¹è¯å°¾éƒ¨ï¼‰
        if history:
            history = self._truncate_history_by_tokens(history, CONTEXT_TOKEN_HARD_LIMIT)

        kwargs = {
            "api_key": self.api_key,
            "model": self.model,
            "messages": history,
            "stream": False,
            "top_p": top_p,
            # é‡è¦ï¼šmetadata ç”¨å¯åºåˆ—åŒ–çš„å­—ç¬¦ä¸²ï¼ˆæšä¸¾ç”¨ .nameï¼Œå­—ç¬¦ä¸²ç›´æ¥ç”¨ï¼‰
            "metadata": {"agent_name": getattr(agent_name, "name", str(agent_name))},
            "request_timeout": REQUEST_TIMEOUT,  # æ•´ä½“è¯·æ±‚è¶…æ—¶
            "client_args": {"timeout": HTTPX_TIMEOUTS},  # httpx ç»†ç²’åº¦è¶…æ—¶
        }

        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = tool_choice

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        if self.base_url:
            kwargs["base_url"] = self.base_url

        # TODO: stream è¾“å‡ºï¼ˆå¦‚éœ€æµå¼ï¼Œæ³¨æ„æ¥æ”¶æµæ—¶çš„å®¹é”™ä¸å–æ¶ˆå¤„ç†ï¼‰
        for attempt in range(max_retries):
            try:
                response = await acompletion(**kwargs)
                logger.info(f"APIè¿”å›: {response}")

                # åŸºæœ¬æ ¡éªŒ
                if not response or not hasattr(response, "choices"):
                    raise ValueError("æ— æ•ˆçš„APIå“åº”")

                self.chat_count += 1
                await self.send_message(response, agent_name, sub_title)
                return response

            except asyncio.CancelledError:
                logger.warning("è¯·æ±‚è¢«ä¸Šå±‚å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œä¸é‡è¯•ã€‚")
                raise

            # â€”â€” ä¸é‡è¯•çš„ 4xx é€»è¾‘é”™è¯¯/é…ç½®é”™è¯¯ â€”â€”
            except (
                litellm.BadRequestError,
                litellm.AuthenticationError,
                litellm.NotFoundError,
            ) as e:
                msg = str(e)
                if "maximum context length" in msg or "context length" in msg or "ContextWindowExceeded" in msg:
                    logger.error("éé‡è¯•é”™è¯¯ï¼šä¸Šä¸‹æ–‡è¶…é™ï¼Œè¯·ç¡®ä¿åœ¨è¿›å…¥ acompletion å‰å·²å……åˆ†æˆªæ–­ã€‚")
                else:
                    logger.error(f"éé‡è¯•é”™è¯¯ï¼š{e}")
                raise

            # â€”â€” å¯é‡è¯•çš„é”™è¯¯ï¼šç½‘ç»œ/é™æµ/è¶…æ—¶/5xx/å¶å‘è§£æ â€”â€”
            except (
                litellm.RateLimitError,
                litellm.Timeout,
                litellm.APIConnectionError,
                litellm.InternalServerError,
                json.JSONDecodeError,
            ) as e:
                logger.error(f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•: {e}")
                if attempt >= max_retries - 1:
                    logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                    raise
                delay = retry_delay * (2**attempt) + random.random() * 0.3
                await asyncio.sleep(delay)

            # â€”â€” å…œåº•ï¼šæœªçŸ¥å¼‚å¸¸ï¼Œå°‘é‡é‡è¯• â€”â€”
            except Exception as e:
                logger.error(f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•ï¼ˆæœªçŸ¥å¼‚å¸¸ï¼‰: {e}")
                if attempt >= max_retries - 1:
                    logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                    raise
                delay = retry_delay * (2**attempt) + random.random() * 0.3
                await asyncio.sleep(delay)

    def _validate_and_fix_tool_calls(self, history: list) -> list:
        """éªŒè¯å¹¶ä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§"""
        if not history:
            return history

        ic(f"ğŸ” å¼€å§‹éªŒè¯å·¥å…·è°ƒç”¨ï¼Œå†å²æ¶ˆæ¯æ•°é‡: {len(history)}")

        # æŸ¥æ‰¾æ‰€æœ‰æœªåŒ¹é…çš„tool_calls
        fixed_history = []
        i = 0

        while i < len(history):
            msg = history[i]

            # å¦‚æœæ˜¯åŒ…å«tool_callsçš„æ¶ˆæ¯
            if isinstance(msg, dict) and "tool_calls" in msg and msg["tool_calls"]:
                ic(f"ğŸ“ å‘ç°tool_callsæ¶ˆæ¯åœ¨ä½ç½® {i}")

                # æ£€æŸ¥æ¯ä¸ªtool_callæ˜¯å¦éƒ½æœ‰å¯¹åº”çš„responseï¼Œåˆ†åˆ«å¤„ç†
                valid_tool_calls = []
                invalid_tool_calls = []

                for tool_call in msg["tool_calls"]:
                    tool_call_id = tool_call.get("id")
                    ic(f"  æ£€æŸ¥tool_call_id: {tool_call_id}")

                    if tool_call_id:
                        # æŸ¥æ‰¾å¯¹åº”çš„toolå“åº”
                        found_response = False
                        for j in range(i + 1, len(history)):
                            if history[j].get("role") == "tool" and history[j].get("tool_call_id") == tool_call_id:
                                ic(f"  âœ… æ‰¾åˆ°åŒ¹é…å“åº”åœ¨ä½ç½® {j}")
                                found_response = True
                                break

                        if found_response:
                            valid_tool_calls.append(tool_call)
                        else:
                            ic(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…å“åº”: {tool_call_id}")
                            invalid_tool_calls.append(tool_call)

                # æ ¹æ®æ£€æŸ¥ç»“æœå¤„ç†æ¶ˆæ¯
                if valid_tool_calls:
                    # æœ‰æœ‰æ•ˆçš„tool_callsï¼Œä¿ç•™å®ƒä»¬
                    fixed_msg = msg.copy()
                    fixed_msg["tool_calls"] = valid_tool_calls
                    fixed_history.append(fixed_msg)
                    ic(f"  ğŸ”§ ä¿ç•™ {len(valid_tool_calls)} ä¸ªæœ‰æ•ˆtool_callsï¼Œç§»é™¤ {len(invalid_tool_calls)} ä¸ªæ— æ•ˆçš„")
                else:
                    # æ²¡æœ‰æœ‰æ•ˆçš„tool_callsï¼Œç§»é™¤tool_callsä½†å¯èƒ½ä¿ç•™å…¶ä»–å†…å®¹
                    cleaned_msg = {k: v for k, v in msg.items() if k != "tool_calls"}
                    if cleaned_msg.get("content"):
                        fixed_history.append(cleaned_msg)
                        ic(f"  ğŸ”§ ç§»é™¤æ‰€æœ‰tool_callsï¼Œä¿ç•™æ¶ˆæ¯å†…å®¹")
                    else:
                        ic(f"  ğŸ—‘ï¸ å®Œå…¨ç§»é™¤ç©ºçš„tool_callsæ¶ˆæ¯")

            # å¦‚æœæ˜¯toolå“åº”æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å­¤ç«‹çš„
            elif isinstance(msg, dict) and msg.get("role") == "tool":
                tool_call_id = msg.get("tool_call_id")
                ic(f"ğŸ”§ æ£€æŸ¥toolå“åº”æ¶ˆæ¯: {tool_call_id}")

                # æŸ¥æ‰¾å¯¹åº”çš„tool_calls
                found_call = False
                for j in range(len(fixed_history)):
                    if fixed_history[j].get("tool_calls") and any(
                        tc.get("id") == tool_call_id for tc in fixed_history[j]["tool_calls"]
                    ):
                        found_call = True
                        break

                if found_call:
                    fixed_history.append(msg)
                    ic(f"  âœ… ä¿ç•™æœ‰æ•ˆçš„toolå“åº”")
                else:
                    ic(f"  ğŸ—‘ï¸ ç§»é™¤å­¤ç«‹çš„toolå“åº”: {tool_call_id}")

            else:
                # æ™®é€šæ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
                fixed_history.append(msg)

            i += 1

        if len(fixed_history) != len(history):
            ic(f"ğŸ”§ ä¿®å¤å®Œæˆ: {len(history)} -> {len(fixed_history)} æ¡æ¶ˆæ¯")
        else:
            ic(f"âœ… éªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®å¤")

        return fixed_history

    def _truncate_history_by_tokens(self, history: list, token_limit: int) -> list:
        """
        æŒ‰ token æ•°é‡è£å‰ª messagesï¼ˆä¿ç•™é¦–æ¡ system + å°¾éƒ¨è‹¥å¹²æ¡ï¼‰ã€‚
        ä¸ºé¿å…ç ´åå·¥å…·æ¶ˆæ¯é…å¯¹ï¼Œé‡‡ç”¨â€œå–å¯¹è¯å°¾éƒ¨è¿ç»­ç‰‡æ®µâ€çš„ç­–ç•¥ï¼Œå†åšä¸€æ¬¡å®Œæ•´æ€§æ ¡éªŒã€‚
        """
        if not history:
            return history

        # è®¡ç®— token çš„è¾…åŠ©å‡½æ•°
        def msg_tokens(msg: dict) -> int:
            # ä»…å¯¹ content è®¡æ•°ï¼ˆrole/tool_calls å…ƒæ•°æ®ä¸è®¡ï¼‰
            content = msg.get("content") or ""
            try:
                return token_counter(content, self.model)
            except Exception:
                # å…œåº•ä¼°ç®—ï¼ˆå¤§è‡´ 3~4 å­—ç¬¦ ~ 1 tokenï¼‰
                return max(1, len(content) // 3)

        # é¦–æ¡å¯èƒ½æ˜¯ systemï¼Œå°½é‡ä¿ç•™
        system_msg = None
        start_idx = 0
        if history[0].get("role") == "system":
            system_msg = history[0]
            start_idx = 1

        # å…ˆå°è¯•å…¨é‡è®¡æ•°
        total = (msg_tokens(system_msg) if system_msg else 0) + sum(msg_tokens(m) for m in history[start_idx:])
        if total <= token_limit:
            return history

        # ä»å°¾éƒ¨å‘å‰ç´¯ç§¯ï¼Œç›´åˆ°è¾¾åˆ°ä¸Šé™
        kept = []
        running = msg_tokens(system_msg) if system_msg else 0

        for i in range(len(history) - 1, start_idx - 1, -1):
            t = msg_tokens(history[i])
            if running + t > token_limit:
                break
            kept.append(history[i])
            running += t

        kept.reverse()
        new_history = [system_msg] + kept if system_msg else kept

        # å†æ¬¡åšå·¥å…·è°ƒç”¨å®Œæ•´æ€§ä¿®å¤ï¼Œé¿å…äº§ç”Ÿå­¤ç«‹ tool æ¶ˆæ¯
        new_history = self._validate_and_fix_tool_calls(new_history)
        return new_history

    async def send_message(self, response, agent_name, sub_title=None):
        logger.info(f"subtitleæ˜¯:{sub_title}")
        raw_content = response.choices[0].message.content or ""

        # å…è®¸ä¸Šæ¸¸ä¼ å­—ç¬¦ä¸²ï¼ˆå¦‚ "JsonFixerHeavy"ï¼‰ï¼Œåœ¨æ­¤å½’ä¸€åŒ–ä¸º AgentType
        if isinstance(agent_name, str):
            key = agent_name.lower().replace(" ", "")
            mapping = {
                "coordinatoragent": AgentType.COORDINATOR,
                "modeleragent": AgentType.MODELER,
                "writeragent": AgentType.WRITER,
                "coderagent": AgentType.CODER,
                "jsonfixer": AgentType.MODELER,
                "jsonfixerheavy": AgentType.MODELER,
            }
            agent_name = mapping.get(key, None) or (
                AgentType.COORDINATOR
                if "coord" in key
                else (
                    AgentType.MODELER
                    if ("model" in key or "jsonfixer" in key)
                    else (
                        AgentType.WRITER if "writer" in key else AgentType.CODER if "coder" in key else AgentType.SYSTEM
                    )
                )
            )

        # ------- ä»…å¯¹ Coordinator / Modeler åš JSON è§„èŒƒåŒ–ï¼ˆå³ä¾§é¢æ¿è¦åƒå¹²å‡€ JSONï¼‰-------
        def _cleanup_fences(s: str) -> str:
            return (s or "").replace("```json", "").replace("```", "").strip()

        def _cleanup_ctrl(s: str) -> str:
            return re.sub(r"[\x00-\x1F\x7F]", "", s or "")

        def _extract_first_json_block(s: str) -> str:
            if not s:
                return ""
            start = s.find("{")
            if start == -1:
                return ""
            stack, in_str, esc = [], False, False
            for i, ch in enumerate(s[start:], start):
                if in_str:
                    if esc:
                        esc = False
                    elif ch == "\\":
                        esc = True
                    elif ch == '"':
                        in_str = False
                else:
                    if ch == '"':
                        in_str = True
                    elif ch == "{":
                        stack.append("{")
                    elif ch == "}":
                        if stack:
                            stack.pop()
                        if not stack:
                            return s[start : i + 1]
            return ""

        def _normalize_json_for_right_panel(text: str):
            # è¿”å› (ok, normalized_text)
            cleaned = _cleanup_ctrl(_cleanup_fences(text))
            blk = _extract_first_json_block(cleaned)
            if not blk:
                return False, text
            try:
                obj = json.loads(blk)
                return True, json.dumps(obj, ensure_ascii=False)
            except Exception:
                return False, text

        content_to_send = raw_content
        if agent_name in (AgentType.COORDINATOR, AgentType.MODELER):
            ok, normalized = _normalize_json_for_right_panel(raw_content)
            if ok:
                content_to_send = normalized
            else:
                logger.warning("send_message: æœªèƒ½ä»åŸæ–‡ä¸­æå–åˆæ³• JSONï¼ŒæŒ‰åŸæ–‡å‘å¸ƒã€‚")

        # ------- æ„é€ å¹¶å‘å¸ƒå¯¹åº”æ¶ˆæ¯ -------
        match agent_name:
            case AgentType.CODER:
                agent_msg: CoderMessage = CoderMessage(content=content_to_send)
            case AgentType.WRITER:
                # å¤„ç† Markdown å›¾ç‰‡/è„šæ³¨
                c, _ = split_footnotes(content_to_send)
                c = transform_link(self.task_id, c)
                agent_msg: WriterMessage = WriterMessage(content=c, sub_title=sub_title)
            case AgentType.MODELER:
                agent_msg: ModelerMessage = ModelerMessage(content=content_to_send)
            case AgentType.COORDINATOR:
                agent_msg: CoordinatorMessage = CoordinatorMessage(content=content_to_send)
            case AgentType.SYSTEM:
                agent_msg: SystemMessage = SystemMessage(content=content_to_send)
            case _:
                agent_msg: SystemMessage = SystemMessage(content=content_to_send)

        await redis_manager.publish_message(self.task_id, agent_msg)


async def simple_chat(model: LLM, history: list) -> str:
    """
    é‡é‡ç‰ˆ simple_chatï¼š
    1) å…ˆä¿®å¤å·¥å…·æ¶ˆæ¯å®Œæ•´æ€§ï¼ˆé¿å…å­¤ç«‹ tool / æœªåŒ¹é…çš„ tool_callï¼‰
    2) åœ¨æ€» token è¶…é™æ—¶ï¼Œé‡‡ç”¨ï¼šä¿ç•™ system + å°¾éƒ¨å®Œæ•´å¯¹è¯ç‰‡æ®µ + ä¸­æ®µè‡ªåŠ¨æ‘˜è¦
    3) è¿­ä»£å‹ç¼©ï¼Œç›´åˆ° <= CONTEXT_TOKEN_HARD_LIMIT åå†å‘èµ·æœ€ç»ˆè¡¥å…¨
    """

    # ========== å·¥å…·å‡½æ•° ==========
    def quick_count(msg):
        content = (msg or {}).get("content") or ""
        try:
            return token_counter(content, model.model)
        except Exception:
            return max(1, len(content) // 3)  # å…œåº•ä¼°ç®—

    def tokens_of(messages):
        if not messages:
            return 0
        return sum(quick_count(m) for m in messages if isinstance(m, dict))

    def pair_safe_tail(messages):
        """
        è·å–å¯¹è¯å°¾éƒ¨çš„â€œé…å¯¹å®‰å…¨â€è¿ç»­ç‰‡æ®µï¼ˆé¿å…æŠŠ tool_call / tool å“åº”å¯¹æ‹†å¼€ï¼‰ã€‚
        ç­–ç•¥ï¼šä»å°¾éƒ¨å‘å‰å–ï¼Œé‡åˆ°æœ‰ tool_calls çš„æ¶ˆæ¯ï¼Œå°±ç¡®ä¿å…¶å¯¹åº”çš„ tool å“åº”åœ¨ç‰‡æ®µä¸­ï¼›åä¹‹äº¦ç„¶ã€‚
        è¿™é‡Œç”¨ç®€åŒ–ç­–ç•¥ï¼šå…ˆå–ä¸€æ®µå°¾éƒ¨ï¼Œå†è°ƒç”¨æ¨¡å‹å·²æœ‰çš„ _validate_and_fix_tool_calls ä¿®å¤ã€‚
        """
        # å…ˆç›´æ¥ç»™å‡ºä¸€ä¸ªâ€œè¶³é‡å°¾éƒ¨â€ï¼ˆä»¥æ¶ˆæ¯æ•°ä¸ºå°ºåº¦ï¼Œåç»­å†åš token é™åˆ¶ï¼‰
        MAX_TAIL_MSGS = 30
        start = max(0, len(messages) - MAX_TAIL_MSGS)
        tail = messages[start:]
        # ä¿®å¤å°¾éƒ¨ç‰‡æ®µçš„å·¥å…·å…³ç³»
        return model._validate_and_fix_tool_calls(tail)

    async def summarize_chunk(chunk_msgs):
        """
        ä½¿ç”¨æ¨¡å‹å¯¹ä¸­æ®µè¿›è¡Œæ‘˜è¦å‹ç¼©ï¼ˆé™„å¸¦ system æç¤ºï¼‰ï¼Œè¾“å‡ºå°½é‡ç®€çŸ­ä½†ä¿ç•™å…³é”®ä¿¡æ¯ã€‚
        """
        sys_prompt = {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦å™¨ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å‹ç¼©ä¸ºä¸€æ®µç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œ"
                "ä¿ç•™ä»»åŠ¡ç›®æ ‡ã€å…³é”®çº¦æŸã€é‡è¦ç»“è®ºå’Œå·²å®Œæˆæ­¥éª¤ï¼Œå»é™¤æ— å…³ç»†èŠ‚ã€‚"
                "è¾“å‡ºä¸è¶…è¿‡ 300~500 å­—ã€‚"
            ),
        }
        user_prompt = {
            "role": "user",
            "content": "\n".join(
                f"{m.get('role')}: { (m.get('content') or '')[:2000] }" for m in chunk_msgs if isinstance(m, dict)
            ),
        }

        kwargs = {
            "api_key": model.api_key,
            "model": model.model,
            "messages": [sys_prompt, user_prompt],
            "stream": False,
            "request_timeout": REQUEST_TIMEOUT,
            "client_args": {"timeout": HTTPX_TIMEOUTS},
        }
        if model.base_url:
            kwargs["base_url"] = model.base_url

        resp = await acompletion(**kwargs)
        return resp.choices[0].message.content.strip()

    # ========== é¢„å¤„ç†ï¼šå·¥å…·å®Œæ•´æ€§ä¿®å¤ ==========
    history = history or []
    history = model._validate_and_fix_tool_calls(history)

    # æ‹†å‡º systemï¼ˆè‹¥å­˜åœ¨åˆ™ä¿ç•™ï¼‰
    sys_msg = history[0] if (history and history[0].get("role") == "system") else None
    start_idx = 1 if sys_msg else 0
    body = history[start_idx:]

    # å¿«é€Ÿé€šè¿‡ï¼šæœªè¶…é™ç›´æ¥è¯·æ±‚
    total_tokens = (quick_count(sys_msg) if sys_msg else 0) + tokens_of(body)
    if total_tokens <= CONTEXT_TOKEN_HARD_LIMIT:
        kwargs = {
            "api_key": model.api_key,
            "model": model.model,
            "messages": history,
            "stream": False,
            "request_timeout": REQUEST_TIMEOUT,
            "client_args": {"timeout": HTTPX_TIMEOUTS},
        }
        if model.base_url:
            kwargs["base_url"] = model.base_url
        resp = await acompletion(**kwargs)
        return resp.choices[0].message.content

    # ========== é‡é‡å‹ç¼©æµç¨‹ ==========
    # ç›®æ ‡ï¼šsystem + [æ‘˜è¦åçš„ä¸­æ®µ1æ¡] + å®‰å…¨å°¾éƒ¨ç‰‡æ®µ  â€”â€”> è¿­ä»£åˆ° <= é¢„ç®—
    # æ­¥éª¤ï¼š
    # 1) å…ˆæ‹¿ä¸€ä¸ªâ€œé…å¯¹å®‰å…¨çš„å°¾éƒ¨ç‰‡æ®µâ€ tail
    # 2) å…¶ä½™ä½œä¸º headï¼ˆéœ€è¦æ‘˜è¦ï¼‰
    # 3) ç”¨ summarize_chunk(head) å¾—åˆ°ä¸€æ¡ç®€çŸ­ assistant æ¶ˆæ¯
    # 4) ç»„åˆ new_history = [sys?] + [summary_msg] + tail -> è‹¥ä»è¶…é™ï¼Œç¼©çŸ­ tail å†æ‘˜è¦ï¼ˆæˆ–äºŒæ¬¡æ‘˜è¦ï¼‰
    MAX_SUMMARY_ROUNDS = 3
    for round_idx in range(MAX_SUMMARY_ROUNDS):
        tail = pair_safe_tail(body)
        # é¢„ç®—è¦ç•™ç»™ï¼šsystem + summary(â‰ˆ500å­—) + tail
        # å…ˆä¼°è®¡ summary é¢„ç®—ï¼šç”¨ quick_count çš„ç²—ç•¥å€¼ï¼Œç»™ 1500 tokens ä½™é‡è¾ƒç¨³å¦¥
        SUMMARY_BUDGET_HINT = 1500

        # äºŒåˆ†/çº¿æ€§ç¼©çŸ­ tailï¼Œç›´åˆ°â€œsystem + é¢„ä¼°summary + tailâ€ä¸è¶…è¿‡ä¸Šé™ï¼ˆç²—ç­›ï¼‰
        def tail_tokens(t):
            return tokens_of(t)

        keep = len(tail)
        while keep > 0:
            candidate_tail = tail[-keep:]
            rough_total = (quick_count(sys_msg) if sys_msg else 0) + SUMMARY_BUDGET_HINT + tail_tokens(candidate_tail)
            if rough_total <= CONTEXT_TOKEN_HARD_LIMIT:
                tail = model._validate_and_fix_tool_calls(candidate_tail)
                break
            keep //= 2
        else:
            # å®åœ¨å¤ªæ»¡ï¼Œå°¾éƒ¨æ¸…ç©ºï¼Œä»…ä¿ç•™æ‘˜è¦
            tail = []

        # éœ€è¦æ‘˜è¦çš„ head = body å»æ‰ tail çš„å‰ç¼€éƒ¨åˆ†
        cut_at = len(body) - len(tail)
        head = body[: max(cut_at, 0)]

        # åšä¸€æ¬¡æ‘˜è¦ï¼ˆè‹¥ head ä¸ºç©ºåˆ™ç”¨ç©ºæ‘˜è¦ï¼‰
        summary_text = ""
        if head:
            try:
                summary_text = await summarize_chunk(head)
            except Exception as e:
                logger.error(f"æ‘˜è¦å¤±è´¥ï¼Œå›é€€ä½¿ç”¨ç®€çŸ­å ä½ï¼š{e}")
                summary_text = "ï¼ˆå¯¹è¯ä¸­æ®µæ‘˜è¦ï¼šåŒ…å«è‹¥å¹²æ­¥éª¤ã€é”™è¯¯ä¿®å¤ä¸ä¸­é—´ç»“è®ºï¼Œå·²çœç•¥ç»†èŠ‚ä»¥èŠ‚çœä¸Šä¸‹æ–‡ã€‚ï¼‰"
        summary_msg = {"role": "assistant", "content": f"[å†å²å¯¹è¯æ€»ç»“] {summary_text}"}

        new_history = ([sys_msg] if sys_msg else []) + [summary_msg] + tail
        new_history = model._validate_and_fix_tool_calls(new_history)

        # ç²¾ç¡®æ£€æŸ¥ token
        exact_total = tokens_of(new_history)
        if exact_total <= CONTEXT_TOKEN_HARD_LIMIT:
            # è¾¾æ ‡ï¼Œæœ€ç»ˆè¯·æ±‚
            kwargs = {
                "api_key": model.api_key,
                "model": model.model,
                "messages": new_history,
                "stream": False,
                "request_timeout": REQUEST_TIMEOUT,
                "client_args": {"timeout": HTTPX_TIMEOUTS},
            }
            if model.base_url:
                kwargs["base_url"] = model.base_url
            resp = await acompletion(**kwargs)
            return resp.choices[0].message.content

        # è¿˜è¶…ï¼Œä¸Šå†æ¥ä¸€è½®ï¼šè¿›ä¸€æ­¥ç¼©å°¾éƒ¨æˆ–äºŒæ¬¡æ‘˜è¦
        body = head + tail  # ç»§ç»­ä»¥â€œæ›´çŸ­çš„å¯å‹ç¼©ä½“â€ä½œä¸ºä¸‹ä¸€è½®è¾“å…¥

    # å¤šè½®ä»è¶…é™ï¼šé€€è€Œæ±‚å…¶æ¬¡ â€”â€” ä»…ä¿ç•™ system + æçŸ­æ‘˜è¦
    try:
        minimal_summary = await summarize_chunk(body[:50])  # ä»…é‡‡æ ·å‰50æ¡åšä¸€ä¸ªæçŸ­æ‘˜è¦
    except Exception:
        minimal_summary = "ï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå·²å‹ç¼©ä¸ºæçŸ­æ‘˜è¦ã€‚ï¼‰"
    final_history = ([sys_msg] if sys_msg else []) + [
        {"role": "assistant", "content": f"[å†å²å¯¹è¯æç®€æ€»ç»“] {minimal_summary}"}
    ]

    kwargs = {
        "api_key": model.api_key,
        "model": model.model,
        "messages": final_history,
        "stream": False,
        "request_timeout": REQUEST_TIMEOUT,
        "client_args": {"timeout": HTTPX_TIMEOUTS},
    }
    if model.base_url:
        kwargs["base_url"] = model.base_url
    resp = await acompletion(**kwargs)
    return resp.choices[0].message.content
