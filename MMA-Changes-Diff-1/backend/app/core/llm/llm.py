# app/core/llm/llm.py

import json
import codecs
import string
import asyncio
import random
import uuid
from typing import Any, List, Dict

from app.utils.common_utils import transform_link, split_footnotes
from app.utils.log_util import logger
from app.schemas.response import (
    CoderMessage,
    WriterMessage,
    ModelerMessage,
    SystemMessage,
    CoordinatorMessage,
)
from app.services.redis_manager import redis_manager
from litellm import acompletion, token_counter
import litellm
from app.schemas.enums import AgentType
from app.utils.track import agent_metrics
from icecream import ic

# å¼•å…¥é›†ä¸­å¼æ¸…æ´—/æ­£åˆ™å·¥å…·
from app.tools.text_sanitizer import TextSanitizer as TS
from app.tools.json_fixer import JsonFixer

# ====== å…¨å±€ï¼šè¯·æ±‚ä¸é‡è¯•é…ç½®ï¼ˆå¯æŒ‰éœ€è°ƒå¤§/è°ƒå°ï¼‰======
REQUEST_TIMEOUT = 300.0  # å•æ¬¡è¯·æ±‚æ•´ä½“è¶…æ—¶ï¼ˆç§’ï¼‰
HTTPX_TIMEOUTS = {
    "connect": 120,
    "read": 60,
    "write": 120,
    "pool": 60,
}
DEFAULT_MAX_RETRIES = 100
BACKOFF_BASE = 0.8  # æŒ‡æ•°é€€é¿åŸºæ•°ï¼Œå®é™… backoff = base * (2**attempt) + jitter

# ====== ä¸Šä¸‹æ–‡é•¿åº¦ä¿æŠ¤ï¼ˆç»™ DeepSeek/GPT ç­‰ç•™ä½™é‡ï¼‰======
# æ¨¡å‹æ ‡ç§°æœ€å¤§ 131072ï¼Œè¿™é‡Œä¿å®ˆé™åˆ¶åœ¨ 120000 å·¦å³ï¼Œé¿å…è§¦å‘ 400
CONTEXT_TOKEN_HARD_LIMIT = 120_000

litellm.callbacks = [agent_metrics]

# ========= æœ€åä¸€è·³æ¶ˆæ¯æ¸…æ´—ï¼ˆç¡®ä¿ messages å¯è¢« OpenAI/DeepSeek æ­£ç¡®ååºåˆ—åŒ–ï¼‰ =========
# ä»…ä¿ç•™é¡¶å±‚å…è®¸çš„å­—æ®µï¼šrole/content/name/tool_calls/tool_call_id
_ALLOWED_KEYS = {"role", "content", "name", "tool_calls", "tool_call_id"}


def _json_dumps_safe(obj: Any) -> str:
    try:
        return json.dumps(obj, ensure_ascii=False)
    except Exception:
        return str(obj)


def _extract_tool_text(msg: Dict[str, Any]) -> str:
    """å°½é‡ä» tool æ¶ˆæ¯çš„å„ç±»å­—æ®µä¸­æç‚¼å¯è¯»æ–‡æœ¬"""
    extracted = []

    # å¸¸è§ï¼šoutput/outputs/result/results
    out = msg.get("output") or msg.get("outputs") or msg.get("result") or msg.get("results")
    if out is not None:
        if isinstance(out, (list, tuple)):
            for it in out:
                if isinstance(it, dict):
                    for k in ("msg", "message", "text", "result", "content"):
                        v = it.get(k)
                        if v:
                            extracted.append(str(v))
                            break
                    else:
                        extracted.append(_json_dumps_safe(it))
                else:
                    extracted.append(str(it))
        elif isinstance(out, dict):
            for k in ("msg", "message", "text", "result", "content"):
                v = out.get(k)
                if v:
                    extracted.append(str(v))
                    break
            else:
                extracted.append(_json_dumps_safe(out))
        else:
            extracted.append(str(out))

    # å¤‡é€‰ï¼štext/stdout/stderr/data/value
    for k in ("text", "stdout", "stderr", "data", "value"):
        v = msg.get(k)
        if v:
            if isinstance(v, (list, dict)):
                extracted.append(_json_dumps_safe(v))
            else:
                extracted.append(str(v))

    # å¤‡é€‰ï¼štool_result/tool_response/tool_outputs
    tc = msg.get("tool_result") or msg.get("tool_response") or msg.get("tool_outputs")
    if tc is not None:
        extracted.append(_json_dumps_safe(tc))

    # å»é‡+æ‹¼æ¥
    parts, seen = [], set()
    for s in (x.strip() for x in extracted if isinstance(x, str)):
        if s and s not in seen:
            seen.add(s)
            parts.append(s)
    return "\n".join(parts)


def _looks_like_literal_escapes(s: str) -> bool:
    """
    å§”æ‰˜ç»™ TextSanitizer çš„å®ç°ï¼Œä¿æŒè¡Œä¸ºä¸€è‡´ã€‚
    """
    return TS.looks_like_literal_escapes(s)


def _stringify_tool_calls(tc_list: Any) -> Any:
    """æŠŠ assistant æ¶ˆæ¯é‡Œçš„ tool_calls.arguments å¼ºåˆ¶è½¬æˆå­—ç¬¦ä¸²ï¼Œå¹¶å…œåº• function.name / type / id"""
    if not isinstance(tc_list, (list, tuple)):
        return tc_list
    cleaned = []
    for tc in tc_list:
        if not isinstance(tc, dict):
            cleaned.append(tc)
            continue

        tc = dict(tc)

        # type å…œåº•
        if tc.get("type") != "function":
            tc["type"] = "function"

        # id å…œåº•ï¼ˆè‹¥ä¸Šæ¸¸ä¸ç»™ï¼Œæˆ‘ä»¬è‡ªå·±ç»™ï¼Œåç»­ tool æ¶ˆæ¯ç”¨åŒä¸€ä¸ª idï¼‰
        if not isinstance(tc.get("id"), str) or not tc.get("id"):
            tc["id"] = f"call_{uuid.uuid4().hex[:12]}"

        # function å…œåº•
        fn = tc.get("function") or {}
        if not isinstance(fn, dict):
            fn = {"name": "unknown", "arguments": _json_dumps_safe(fn)}
        name = fn.get("name")
        if not isinstance(name, str) or not name:
            name = "unknown"
        args = fn.get("arguments")
        if isinstance(args, (dict, list)):
            args = _json_dumps_safe(args)
        if args is None:
            args = ""

        tc["function"] = {"name": name, "arguments": args}
        cleaned.append(tc)
    return cleaned


def sanitize_messages_for_openai(history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æœ€åä¸€è·³å¼ºåˆ¶æ¸…æ´—ï¼ˆOpenAI/DeepSeek å…¼å®¹ï¼‰æ”¹è‰¯ç‰ˆï¼šä¿ç•™åŸæœ‰è¡Œä¸ºã€‚
    1) ä¸ä¼šæ— è„‘è½¬ä¹‰æˆ–ç§»é™¤æ¢è¡Œ
    2) å¦‚æœ content æ˜¯ list-of-linesï¼Œä¿ç•™å¹¶åˆå¹¶ä¸ºçœŸå®æ¢è¡Œ
    3) è‹¥ content ä¸ºå­—ç¬¦ä¸²ä¸”ä»…åŒ…å«å­—é¢ '\\n'ï¼ˆæ²¡æœ‰çœŸå® '\n'ï¼‰ï¼Œåˆ™ä¿å®ˆåœ°å°è¯•ä¸€æ¬¡åè½¬ä¹‰ä¸ºçœŸå®æ¢è¡Œ
    4) å…¶å®ƒè¡Œä¸ºä¸åŸç‰ˆä¸€è‡´ï¼šè§„èŒƒè§’è‰²ã€å¤„ç† assistant.tool_callsã€ç»‘å®š tool_call_idã€ä¸¢å¼ƒå­¤å„¿æ¶ˆæ¯ç­‰
    """
    result: List[Dict[str, Any]] = []
    if not history:
        return result

    pending_tool_ids: List[str] = []  # assistant.tool_calls äº§ç”Ÿçš„å¾…æ¶ˆè´¹ id é˜Ÿåˆ—

    for idx, orig in enumerate(history):
        base = {} if not isinstance(orig, dict) else dict(orig)

        # å…ˆè£å‰ªåˆ°å…è®¸å­—æ®µï¼ˆä¿ç•™åŸ base ç”¨äºæŠ½å–æ–‡æœ¬ï¼‰
        m = {k: v for k, v in base.items() if k in _ALLOWED_KEYS}

        # -------- è§’è‰²è§„èŒƒåŒ– --------
        role = m.get("role") or base.get("role") or "assistant"
        if role == "function":
            role = "tool"
            if not isinstance(m.get("name"), str) or not m.get("name"):
                m["name"] = base.get("name") or "tool"
        elif role == "tool":
            pass
        elif role not in ("system", "user", "assistant"):
            logger.warning(f"[sanitize] unexpected role={role} at idx={idx}, fallback to 'assistant'")
            role = "assistant"
        m["role"] = role

        # -------- assistant.tool_calls å¤„ç† --------
        if role == "assistant" and base.get("tool_calls"):
            tool_calls = _stringify_tool_calls(base.get("tool_calls"))
            m["tool_calls"] = tool_calls
            for tc in tool_calls or []:
                tc_id = (tc or {}).get("id")
                if isinstance(tc_id, str) and tc_id:
                    pending_tool_ids.append(tc_id)

        # -------- content è§„èŒƒåŒ–ï¼ˆæ‰€æœ‰è§’è‰²ï¼‰--------
        content = None
        # Prefer explicit content from m (already filtered); fallback to base variety
        if "content" in m:
            content = m.get("content")
        else:
            # try to find likely textual fields in original object
            if isinstance(base, dict):
                # keep the same priority as _extract_tool_text but don't over-dumps strings
                for k in ("content", "text", "result", "message", "msg"):
                    if k in base and base.get(k) is not None:
                        content = base.get(k)
                        break

        # Normalize content to a single string while preserving existing newlines:
        normalized_content = ""
        if content is None:
            normalized_content = ""
        elif isinstance(content, list):
            # If already list-of-lines, join preserving explicit line breaks.
            # Allow items to be either strings or objects (non-strings -> json dumps)
            parts = []
            for it in content:
                if isinstance(it, str):
                    parts.append(it)
                else:
                    parts.append(_json_dumps_safe(it))
            normalized_content = "\n".join(parts)
        elif isinstance(content, dict):
            # dict -> json string (no ascii escape)
            normalized_content = _json_dumps_safe(content)
        else:
            # it's a scalar (likely string or number)
            try:
                normalized_content = str(content)
            except Exception:
                normalized_content = ""

        # ä¿å®ˆåè½¬ä¹‰ï¼šä»…å½“å†…å®¹çœ‹èµ·æ¥æ˜¯â€œå­—é¢è½¬ä¹‰çš„ ASCII æ–‡æœ¬â€æ—¶å†å¤„ç†ï¼›
        # ä¸”å¯¹ tool æ¶ˆæ¯ä¸€å¾‹è·³è¿‡ï¼ˆé¿å…ç ´åä»£ç /äºŒè¿›åˆ¶ï¼‰
        if role != "tool" and _looks_like_literal_escapes(normalized_content):
            try:
                # åªåšä¸€æ¬¡ unicode_escape è§£ç ï¼ˆä¸å†å…ˆ .encode('utf-8')ï¼‰
                candidate = codecs.decode(normalized_content, "unicode_escape")
                # è¦æ±‚è‡³å°‘å‡ºç°çœŸå®æ¢è¡Œï¼Œé¿å…æŠŠæ­£å¸¸æ–‡æœ¬æå
                if "\n" in candidate or "\r\n" in candidate or "\t" in candidate:
                    normalized_content = candidate
                else:
                    normalized_content = (
                        normalized_content.replace("\\r\\n", "\n").replace("\\n", "\n").replace("\\t", "\t")
                    )
            except Exception:
                normalized_content = (
                    normalized_content.replace("\\r\\n", "\n").replace("\\n", "\n").replace("\\t", "\t")
                )

        # Now we have normalized_content with real '\n' where appropriate. Do NOT strip or remove newlines.
        # å¯¹â€œtoolï¼ˆå«é—ç•™ functionï¼‰æ¶ˆæ¯â€å°è¯•ä»å…¶å®ƒå­—æ®µæå–å¯è¯»æ–‡æœ¬ï¼ˆä»…åœ¨ content ä¸ºç©ºæ—¶ï¼‰
        if role == "tool" and (not normalized_content or not normalized_content.strip()):
            extracted = _extract_tool_text(base) if isinstance(base, dict) else ""
            if extracted:
                # _extract_tool_text returns joined parts with "\n" already
                normalized_content = extracted

        # å¯¹â€œassistant ä¸”åŒ…å« tool_callsâ€çš„æ¶ˆæ¯ï¼Œå…è®¸æ²¡æœ‰ contentï¼ˆå¤šæ•°æ¨¡å‹å°±æ˜¯ç©º contentï¼‰
        if not (role == "assistant" and m.get("tool_calls")):
            # å…¶å®ƒè§’è‰²ä¸€å¾‹å†™å› content
            m["content"] = normalized_content

        # -------- ä¸º tool æ¶ˆæ¯ç¡®ä¿ tool_call_id é…å¯¹ --------
        if role == "tool":
            tcid = m.get("tool_call_id")
            if not isinstance(tcid, str) or not tcid:
                if pending_tool_ids:
                    assigned = pending_tool_ids.pop(0)
                    m["tool_call_id"] = assigned
                    logger.debug(f"[sanitize] tool msg auto-bound tool_call_id={assigned} at idx={idx}")
                else:
                    # æ²¡æœ‰å¯åŒ¹é…çš„ idï¼Œå±äºå­¤å„¿å·¥å…·å“åº”ï¼šç›´æ¥ä¸¢å¼ƒï¼Œé¿å…éæ³•æ¶ˆæ¯
                    logger.warning(f"[sanitize] dropping orphan tool message at idx={idx} (no matching tool_call_id)")
                    continue

        # -------- å‰”é™¤ None å€¼ï¼Œé¿å…ä¸¥æ ¼æ ¡éªŒé—®é¢˜ --------
        for k in list(m.keys()):
            if m[k] is None:
                del m[k]

        # -------- ä¸¢å¼ƒçº¯ç©ºæ¶ˆæ¯ï¼ˆé™¤ system å¤–ï¼‰--------
        is_meaningless = (
            (role != "system")
            and (not m.get("content", "") or not m.get("content", "").strip())
            and (not m.get("tool_calls"))
            and (role != "tool" or not (m.get("name") or m.get("tool_call_id")))
        )
        if is_meaningless:
            logger.debug(f"[sanitize] drop empty message at idx={idx}, role={role}")
            continue

        # è®°å½• debugï¼ˆä¿æŒåŸæ¥çš„è°ƒè¯•è¾“å‡ºï¼‰
        if (m.get("content", "") or "") == "":
            logger.debug(f"[sanitize] empty content kept at idx={idx}, role={role}")

        result.append(m)

    # è°ƒè¯•ï¼šæ‰“å°å‰å‡ æ¡ï¼Œç¡®è®¤æ²¡æœ‰ None / å¼‚å¸¸
    try:
        for i, mm in enumerate(result[:4]):
            logger.debug(
                f"[sanitize] #{i}: role={mm.get('role')}, "
                f"type(content)={type(mm.get('content'))}, "
                f"len(content)={len(mm.get('content') or '')}"
            )
    except Exception:
        pass

    return result


# =======================================================================================


class LLM:
    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str,
        task_id: str,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.chat_count = 0
        self.max_tokens: int | None = None
        self.task_id = task_id

    async def chat(
        self,
        history: list = None,
        tools: list = None,
        tool_choice: str = None,
        max_retries: int = DEFAULT_MAX_RETRIES,
        retry_delay: float = BACKOFF_BASE,
        top_p: float | None = None,
        agent_name: AgentType | str = AgentType.SYSTEM,
        sub_title: str | None = None,
        publish: bool = True,  # å…³é”®æ–°å¢ï¼šæ˜¯å¦å‘å¸ƒåˆ° Redis/WebSocket
    ) -> object:  # è¿”å› ModelResponse
        logger.info(f"subtitleæ˜¯:{sub_title}")

        # 1) å·¥å…·é…å¯¹ä¿®å¤ + æˆªæ–­ + systemåé¦–æ¡user
        if history:
            history = self._validate_and_fix_tool_calls(history)
            history = self._truncate_history_by_tokens(history, CONTEXT_TOKEN_HARD_LIMIT)
            history = self._ensure_first_after_system_user(history)

        # 2) æœ€åä¸€è·³æ¸…æ´—
        safe_messages = sanitize_messages_for_openai(history or [])

        # 3) ç»„è£…è¯·æ±‚å‚æ•°
        kwargs = {
            "api_key": self.api_key,
            "model": self.model,
            "messages": safe_messages,
            "stream": False,
            "top_p": top_p,
            "metadata": {"agent_name": getattr(agent_name, "name", str(agent_name))},
            "request_timeout": REQUEST_TIMEOUT,
            "client_args": {"timeout": HTTPX_TIMEOUTS},
        }
        if tools:
            kwargs["tools"] = tools
            if tool_choice is not None:
                kwargs["tool_choice"] = tool_choice
        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens
        if self.base_url:
            kwargs["base_url"] = self.base_url

        # 4) è°ƒç”¨ + é‡è¯•
        for attempt in range(max_retries):
            try:
                response = await acompletion(**kwargs)
                logger.info(f"APIè¿”å›: {response}")

                if not response or not hasattr(response, "choices"):
                    raise ValueError("æ— æ•ˆçš„APIå“åº”")

                # ä»…åœ¨ publish=True æ—¶ï¼Œæ‰å…¥åº“å¹¶å¹¿æ’­
                if publish:
                    self.chat_count += 1
                    await self.send_message(response, agent_name, sub_title)

                return response

            except asyncio.CancelledError:
                logger.warning("è¯·æ±‚è¢«ä¸Šå±‚å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œä¸é‡è¯•ã€‚")
                raise
            except (litellm.BadRequestError, litellm.AuthenticationError, litellm.NotFoundError) as e:
                msg = str(e)
                if "maximum context length" in msg or "context length" in msg or "ContextWindowExceeded" in msg:
                    logger.error("éé‡è¯•é”™è¯¯ï¼šä¸Šä¸‹æ–‡è¶…é™ï¼Œè¯·ç¡®ä¿åœ¨è¿›å…¥ acompletion å‰å·²å……åˆ†æˆªæ–­ã€‚")
                else:
                    logger.error(f"éé‡è¯•é”™è¯¯ï¼š{e}")
                raise
            except (
                litellm.RateLimitError,
                litellm.Timeout,
                litellm.APIConnectionError,
                litellm.InternalServerError,
                json.JSONDecodeError,
            ) as e:
                logger.error(f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•: {e}")
                if attempt >= max_retries - 1:
                    logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                    raise
                delay = retry_delay * (2**attempt) + random.random() * 0.3
                await asyncio.sleep(delay)
            except Exception as e:
                logger.error(f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è¯•ï¼ˆæœªçŸ¥å¼‚å¸¸ï¼‰: {e}")
                if attempt >= max_retries - 1:
                    logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                    raise
                delay = retry_delay * (2**attempt) + random.random() * 0.3
                await asyncio.sleep(delay)

    def _validate_and_fix_tool_calls(self, history: list) -> list:
        """
        éªŒè¯å¹¶ä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§ï¼ˆOpenAI æ–°è§„èŒƒï¼‰ï¼š
        1) åˆæ³•è§’è‰²åªå…è®¸ï¼šsystem / user / assistant / tool
        2) assistant æ¶ˆæ¯é‡Œçš„ tool_calls[*].id å¿…é¡»ä¸åç»­æŸæ¡ role='tool' çš„æ¶ˆæ¯çš„ tool_call_id åŒ¹é…
        3) è‹¥å‘ç°å†å²é—ç•™çš„ role='function'ï¼Œåœ¨æ­¤é˜¶æ®µå°±åœ°æ”¹ä¸º role='tool'
        4) æœªåŒ¹é…åˆ°çš„â€œå­¤å„¿ tool æ¶ˆæ¯â€ä¸¢å¼ƒï¼›assistant ä¸­æœªè¢«æ¶ˆè´¹çš„ tool_calls ä¹Ÿä¼šè¢«ç§»é™¤
        """
        if not history:
            return history

        ic(f"ğŸ” å¼€å§‹éªŒè¯å·¥å…·è°ƒç”¨ï¼Œå†å²æ¶ˆæ¯æ•°é‡: {len(history)}")

        fixed_history = []
        i = 0

        def _is_tool_resp(m: dict) -> bool:
            # å…¼å®¹å†å²ï¼šæŠŠ 'function' è§†ä¸º 'tool' å¹¶åœ¨å†™å…¥æ—¶æ”¹å› 'tool'
            return isinstance(m, dict) and m.get("role") in ("tool", "function")

        while i < len(history):
            msg = history[i]

            # 1) assistant å¸¦ tool_calls çš„æ¶ˆæ¯ï¼šé€ä¸€æ£€æŸ¥æ˜¯å¦æœ‰åç»­å“åº”ï¼ˆtoolï¼‰
            if isinstance(msg, dict) and msg.get("tool_calls"):
                ic(f"ğŸ“ å‘ç°tool_callsæ¶ˆæ¯åœ¨ä½ç½® {i}")
                valid_tool_calls, invalid_tool_calls = [], []

                for tc in msg["tool_calls"]:
                    tool_call_id = (tc or {}).get("id")
                    ic(f"  æ£€æŸ¥tool_call_id: {tool_call_id}")
                    if not tool_call_id:
                        invalid_tool_calls.append(tc)
                        continue

                    found_response = False
                    for j in range(i + 1, len(history)):
                        m2 = history[j]
                        if _is_tool_resp(m2):
                            # è‹¥æ˜¯é—ç•™ 'function'ï¼Œä»…ç”¨äºåˆ¤æ–­ï¼Œç¨åå†™å›ç»Ÿä¸€æ”¹ 'tool'
                            m2_id = m2.get("tool_call_id")
                            if m2_id == tool_call_id:
                                ic(f"  âœ… æ‰¾åˆ°åŒ¹é…å“åº”åœ¨ä½ç½® {j}")
                                found_response = True
                                break

                    if found_response:
                        valid_tool_calls.append(tc)
                    else:
                        ic(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…å“åº”: {tool_call_id}")
                        invalid_tool_calls.append(tc)

                if valid_tool_calls:
                    fixed_msg = msg.copy()
                    fixed_msg["tool_calls"] = valid_tool_calls
                    fixed_history.append(fixed_msg)
                    ic(f"  ğŸ”§ ä¿ç•™ {len(valid_tool_calls)} ä¸ªæœ‰æ•ˆtool_callsï¼Œç§»é™¤ {len(invalid_tool_calls)} ä¸ªæ— æ•ˆçš„")
                else:
                    # æ²¡æœ‰æœ‰æ•ˆ tool_callï¼šå¦‚æœè¿˜æœ‰æ–‡æœ¬ï¼Œå°±ä¿ç•™æ–‡æœ¬ï¼›å¦åˆ™ä¸¢å¼ƒæ•´æ¡
                    cleaned_msg = {k: v for k, v in msg.items() if k != "tool_calls"}
                    content = (cleaned_msg.get("content") or "").strip()
                    if content:
                        fixed_history.append(cleaned_msg)
                        ic(f"  ğŸ”§ ç§»é™¤æ‰€æœ‰tool_callsï¼Œä¿ç•™æ¶ˆæ¯å†…å®¹")
                    else:
                        ic(f"  ğŸ—‘ï¸ å®Œå…¨ç§»é™¤ç©ºçš„tool_callsæ¶ˆæ¯")

            # 2) tool/function å“åº”ï¼šç¡®è®¤æ˜¯å¦ä¸ä¸Šæ¸¸ tool_calls é…å¯¹ï¼›æ— é…å¯¹åˆ™ä¸¢å¼ƒ
            elif _is_tool_resp(msg):
                role = msg.get("role")
                tool_call_id = msg.get("tool_call_id")
                ic(f"ğŸ”§ æ£€æŸ¥å·¥å…·å“åº”æ¶ˆæ¯: role={role}, tool_call_id={tool_call_id}")

                # åœ¨ fixed_history ä¸­å›æº¯æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨åŒ¹é…çš„ assistant.tool_calls
                found_call = False
                for k in range(len(fixed_history) - 1, -1, -1):
                    prev = fixed_history[k]
                    if isinstance(prev, dict) and prev.get("tool_calls"):
                        if any((tc or {}).get("id") == tool_call_id for tc in prev["tool_calls"]):
                            found_call = True
                            break

                if found_call:
                    # ç»Ÿä¸€å°†é—ç•™çš„ 'function' æ”¹ä¸º 'tool'ï¼Œä¸ OpenAI è§„èŒƒä¸€è‡´
                    if role == "function":
                        msg = dict(msg)
                        msg["role"] = "tool"
                    fixed_history.append(msg)
                    ic(f"  âœ… ä¿ç•™æœ‰æ•ˆçš„å·¥å…·å“åº”ï¼ˆrole=toolï¼‰")
                else:
                    ic(f"  ğŸ—‘ï¸ ç§»é™¤å­¤ç«‹çš„å·¥å…·å“åº”: {tool_call_id}")

            else:
                # æ™®é€šæ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
                fixed_history.append(msg)

            i += 1

        if len(fixed_history) != len(history):
            ic(f"ğŸ”§ ä¿®å¤å®Œæˆ: {len(history)} -> {len(fixed_history)} æ¡æ¶ˆæ¯")
        else:
            ic(f"âœ… éªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®å¤")

        return fixed_history

    def _truncate_history_by_tokens(self, history: list, token_limit: int) -> list:
        """
        æŒ‰ token æ•°é‡è£å‰ª messagesï¼ˆä¿ç•™é¦–æ¡ system + å°¾éƒ¨è‹¥å¹²æ¡ï¼‰ã€‚
        ä¸ºé¿å…ç ´åå·¥å…·æ¶ˆæ¯é…å¯¹ï¼Œé‡‡ç”¨â€œå–å¯¹è¯å°¾éƒ¨è¿ç»­ç‰‡æ®µâ€çš„ç­–ç•¥ï¼Œå†åšä¸€æ¬¡å®Œæ•´æ€§æ ¡éªŒã€‚
        """
        if not history:
            return history

        # è®¡ç®— token çš„è¾…åŠ©å‡½æ•°
        def msg_tokens(msg: dict) -> int:
            # ä»…å¯¹ content è®¡æ•°ï¼ˆrole/tool_calls å…ƒæ•°æ®ä¸è®¡ï¼‰
            content = msg.get("content") or ""
            try:
                return token_counter(content, self.model)
            except Exception:
                # å…œåº•ä¼°ç®—ï¼ˆå¤§è‡´ 3~4 å­—ç¬¦ ~ 1 tokenï¼‰
                return max(1, len(content) // 3)

        # é¦–æ¡å¯èƒ½æ˜¯ systemï¼Œå°½é‡ä¿ç•™
        system_msg = None
        start_idx = 0
        if history[0].get("role") == "system":
            system_msg = history[0]
            start_idx = 1

        # å…ˆå°è¯•å…¨é‡è®¡æ•°
        total = (msg_tokens(system_msg) if system_msg else 0) + sum(msg_tokens(m) for m in history[start_idx:])
        if total <= token_limit:
            return history

        # ä»å°¾éƒ¨å‘å‰ç´¯ç§¯ï¼Œç›´åˆ°è¾¾åˆ°ä¸Šé™
        kept = []
        running = msg_tokens(system_msg) if system_msg else 0

        for i in range(len(history) - 1, start_idx - 1, -1):
            t = msg_tokens(history[i])
            if running + t > token_limit:
                break
            kept.append(history[i])
            running += t

        kept.reverse()
        new_history = [system_msg] + kept if system_msg else kept

        # å†æ¬¡åšå·¥å…·è°ƒç”¨å®Œæ•´æ€§ä¿®å¤ï¼Œé¿å…äº§ç”Ÿå­¤ç«‹ tool æ¶ˆæ¯
        new_history = self._validate_and_fix_tool_calls(new_history)
        return new_history

    async def send_message(self, response, agent_name, sub_title=None):
        logger.info(f"subtitleæ˜¯:{sub_title}")
        raw_content = getattr(response.choices[0].message, "content", "") or ""

        # å­—ç¬¦ä¸² -> AgentType çš„å½’ä¸€åŒ–ï¼ˆä¿æŒä½ ç°æœ‰é€»è¾‘ï¼‰
        if isinstance(agent_name, str):
            key = agent_name.lower().replace(" ", "")
            mapping = {
                "coordinatoragent": AgentType.COORDINATOR,
                "modeleragent": AgentType.MODELER,
                "writeragent": AgentType.WRITER,
                "coderagent": AgentType.CODER,
                "jsonfixer": AgentType.MODELER,
                "jsonfixerheavy": AgentType.MODELER,
            }
            agent_name = mapping.get(key, None) or (
                AgentType.COORDINATOR
                if "coord" in key
                else (
                    AgentType.MODELER
                    if ("model" in key or "jsonfixer" in key)
                    else (
                        AgentType.WRITER if "writer" in key else AgentType.CODER if "coder" in key else AgentType.SYSTEM
                    )
                )
            )

        # ------- å¯¹ Coordinator / Modeler åšä¸¥æ ¼ JSON è§„èŒƒåŒ–ï¼ˆå³ä¾§é¢æ¿è¦åƒå¹²å‡€ JSONï¼‰ -------
        content_to_send = raw_content

        if agent_name in (AgentType.COORDINATOR, AgentType.MODELER):
            stripped = TS.strip_fences_outer_or_all(raw_content)
            try:
                # å…³é”®ï¼šæŠŠ llm=self äº¤ç»™ JsonFixerï¼Œç”±å®ƒå†…éƒ¨ç”¨ publish=False è°ƒ self.chat
                obj, stage = await JsonFixer.fix_and_parse(
                    stripped,
                    llm=self,
                    agent_name=f"{getattr(agent_name, 'name', str(agent_name))}.JsonFixer",
                )
            except Exception as e:
                logger.exception(f"JsonFixer è°ƒç”¨å¤±è´¥: {e}")
                err_obj = {"error": "jsonfixer_exception", "exc": str(e)}
                content_to_send = json.dumps(err_obj, ensure_ascii=False)
            else:
                if isinstance(obj, dict):
                    # æˆåŠŸï¼šå‘å¸ƒçº¯ JSON å­—ç¬¦ä¸²ï¼ˆä»…ä¸€å±‚åºåˆ—åŒ–ï¼‰ï¼Œä¸è¦å†åŒ… ```json å›´æ 
                    content_to_send = json.dumps(obj, ensure_ascii=False)
                else:
                    # è§£æå¤±è´¥ï¼šå‘å¸ƒç»“æ„åŒ–é”™è¯¯å¯¹è±¡ï¼ˆé¿å…æŠŠè„åŸæ–‡å†æ¬¡ä¼ å›å¼•èµ·å¾ªç¯ï¼‰
                    preview = (stripped[:600] + "â€¦") if len(stripped) > 600 else stripped
                    err_obj = {"error": "json_unparseable", "stage": stage, "raw_preview": preview}
                    content_to_send = json.dumps(err_obj, ensure_ascii=False)
                    logger.warning(f"send_message: JSON è§£æå¤±è´¥ stage={stage}; å·²å‘å¸ƒé”™è¯¯å¯¹è±¡ä¾›ä¸Šæ¸¸å¤„ç†.")

        # å‘å¸ƒç»™å‰ç«¯ï¼ˆä¿æŒåŸæœ‰åˆ†æ”¯ï¼‰
        match agent_name:
            case AgentType.CODER:
                agent_msg: CoderMessage = CoderMessage(content=content_to_send)
            case AgentType.WRITER:
                c, _ = split_footnotes(content_to_send)
                c = transform_link(self.task_id, c)
                agent_msg: WriterMessage = WriterMessage(content=c, sub_title=sub_title)
            case AgentType.MODELER:
                agent_msg: ModelerMessage = ModelerMessage(content=content_to_send)
            case AgentType.COORDINATOR:
                agent_msg: CoordinatorMessage = CoordinatorMessage(content=content_to_send)
            case AgentType.SYSTEM:
                agent_msg: SystemMessage = SystemMessage(content=content_to_send)
            case _:
                agent_msg: SystemMessage = SystemMessage(content=content_to_send)

        await redis_manager.publish_message(self.task_id, agent_msg)

    def _ensure_first_after_system_user(self, history: list) -> list:
        """
        ä¿è¯ï¼šä»»æ„æ•°é‡çš„ system ä¹‹åï¼Œç¬¬ä¸€æ¡é system å¿…é¡»æ˜¯ userã€‚
        1) è‹¥é¦–æ¡é system æ˜¯ assistant ä¸”å†…å®¹åƒâ€œå†å²å¯¹è¯æ€»ç»“â€¦â€ï¼Œåˆ™å°±åœ°æ”¹æˆ userï¼›
        2) å¦åˆ™åœ¨å…¶å‰é¢æ’å…¥ä¸€æ¡ç®€çŸ­çš„ user æ‰¿æ¥æ¶ˆæ¯ï¼›
        3) è‹¥å…¨æ˜¯ systemï¼ˆæˆ–ç©ºï¼‰ï¼Œä¹Ÿæ’å…¥ä¸€æ¡æœ€å° user å¯åŠ¨è¯­ã€‚
        """
        if not history:
            return [{"role": "user", "content": "[ç©ºå¯¹è¯å¯åŠ¨] ç»§ç»­ã€‚"}]

        # æ‰¾åˆ°é¦–ä¸ªé system çš„ç´¢å¼•
        i = 0
        while i < len(history) and isinstance(history[i], dict) and history[i].get("role") == "system":
            i += 1

        # æƒ…å†µAï¼šå…¨æ˜¯ system
        if i >= len(history):
            return history + [{"role": "user", "content": "[æ‰¿æ¥ä¸Šæ–‡ä¸Šä¸‹æ–‡] ç»§ç»­ã€‚"}]

        # æƒ…å†µBï¼šé¦–ä¸ªé system ä¸æ˜¯ user
        first = history[i] if isinstance(history[i], dict) else {}
        role = first.get("role")
        if role != "user":
            content = (first.get("content") or "").strip()
            # å¦‚æœåƒæˆ‘ä»¬çš„â€œå†å²å¯¹è¯æ€»ç»“â€¦â€ï¼Œç›´æ¥å°±åœ°æ”¹æˆ user æ›´è‡ªç„¶
            if role == "assistant" and content.startswith("[å†å²å¯¹è¯æ€»ç»“"):
                first["role"] = "user"
                history[i] = first
            else:
                # å¦åˆ™åœ¨å…¶å‰é¢æ’å…¥ä¸€æ¡æœ€å° user æ‰¿æ¥æ¶ˆæ¯
                history = history[:i] + [{"role": "user", "content": "[æ‰¿æ¥ä¸Šæ–‡ä¸Šä¸‹æ–‡] ç»§ç»­ã€‚"}] + history[i:]

        return history


async def simple_chat(model: LLM, history: list) -> str:
    """
    é‡é‡ç‰ˆ simple_chatï¼š
    1) å…ˆä¿®å¤å·¥å…·æ¶ˆæ¯å®Œæ•´æ€§ï¼ˆé¿å…å­¤ç«‹ tool / æœªåŒ¹é…çš„ tool_callï¼‰
    2) åœ¨æ€» token è¶…é™æ—¶ï¼Œé‡‡ç”¨ï¼šä¿ç•™ system + å°¾éƒ¨å®Œæ•´å¯¹è¯ç‰‡æ®µ + ä¸­æ®µè‡ªåŠ¨æ‘˜è¦
    3) è¿­ä»£å‹ç¼©ï¼Œç›´åˆ° <= CONTEXT_TOKEN_HARD_LIMIT åå†å‘èµ·æœ€ç»ˆè¡¥å…¨
    """

    def quick_count(msg):
        content = (msg or {}).get("content") or ""
        try:
            return token_counter(content, model.model)
        except Exception:
            return max(1, len(content) // 3)

    def tokens_of(messages):
        if not messages:
            return 0
        return sum(quick_count(m) for m in messages if isinstance(m, dict))

    def pair_safe_tail(messages):
        MAX_TAIL_MSGS = 100
        start = max(0, len(messages) - MAX_TAIL_MSGS)
        tail = messages[start:]
        return model._validate_and_fix_tool_calls(tail)

    async def summarize_chunk(chunk_msgs):
        sys_prompt = {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦å™¨ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å‹ç¼©ä¸ºä¸€æ®µç®€æ´çš„ä¸­æ–‡æ€»ç»“ï¼Œ"
                "ä¿ç•™ä»»åŠ¡ç›®æ ‡ã€å…³é”®çº¦æŸã€é‡è¦ç»“è®ºå’Œå·²å®Œæˆæ­¥éª¤ï¼Œå»é™¤æ— å…³ç»†èŠ‚ã€‚"
                "è¾“å‡ºä¸è¶…è¿‡ 300~600 å­—ã€‚"
            ),
        }
        user_prompt = {
            "role": "user",
            "content": "\n".join(
                f"{m.get('role')}: { (m.get('content') or '')[:2000] }" for m in chunk_msgs if isinstance(m, dict)
            ),
        }
        msgs = sanitize_messages_for_openai([sys_prompt, user_prompt])
        kwargs = {
            "api_key": model.api_key,
            "model": model.model,
            "messages": msgs,
            "stream": False,
            "request_timeout": REQUEST_TIMEOUT,
            "client_args": {"timeout": HTTPX_TIMEOUTS},
        }
        if model.base_url:
            kwargs["base_url"] = model.base_url

        resp = await acompletion(**kwargs)
        return resp.choices[0].message.content.strip()

    # ========== é¢„å¤„ç†ï¼šå·¥å…·å®Œæ•´æ€§ä¿®å¤ ==========
    history = history or []
    history = model._validate_and_fix_tool_calls(history)

    # æ‹†å‡º systemï¼ˆè‹¥å­˜åœ¨åˆ™ä¿ç•™ï¼‰
    sys_msg = history[0] if (history and history[0].get("role") == "system") else None
    start_idx = 1 if sys_msg else 0
    body = history[start_idx:]

    # å¿«é€Ÿé€šè¿‡ï¼šæœªè¶…é™ç›´æ¥è¯·æ±‚
    total_tokens = (quick_count(sys_msg) if sys_msg else 0) + tokens_of(body)
    if total_tokens <= CONTEXT_TOKEN_HARD_LIMIT:
        # **ä¿è¯ system åç¬¬ä¸€æ¡æ˜¯ user**
        ready = [sys_msg] + body if sys_msg else body
        ready = model._ensure_first_after_system_user(ready)
        msgs = sanitize_messages_for_openai(ready)

        kwargs = {
            "api_key": model.api_key,
            "model": model.model,
            "messages": msgs,
            "stream": False,
            "request_timeout": REQUEST_TIMEOUT,
            "client_args": {"timeout": HTTPX_TIMEOUTS},
        }
        if model.base_url:
            kwargs["base_url"] = model.base_url
        resp = await acompletion(**kwargs)
        return resp.choices[0].message.content

    # ========== é‡é‡å‹ç¼©æµç¨‹ ==========
    MAX_SUMMARY_ROUNDS = 3
    for round_idx in range(MAX_SUMMARY_ROUNDS):
        tail = pair_safe_tail(body)
        SUMMARY_BUDGET_HINT = 1500

        def tail_tokens(t):
            return tokens_of(t)

        keep = len(tail)
        while keep > 0:
            candidate_tail = tail[-keep:]
            rough_total = (quick_count(sys_msg) if sys_msg else 0) + SUMMARY_BUDGET_HINT + tail_tokens(candidate_tail)
            if rough_total <= CONTEXT_TOKEN_HARD_LIMIT:
                tail = model._validate_and_fix_tool_calls(candidate_tail)
                break
            keep //= 2
        else:
            tail = []

        cut_at = len(body) - len(tail)
        head = body[: max(cut_at, 0)]

        summary_text = ""
        if head:
            try:
                summary_text = await summarize_chunk(head)
            except Exception as e:
                logger.error(f"æ‘˜è¦å¤±è´¥ï¼Œå›é€€ä½¿ç”¨ç®€çŸ­å ä½ï¼š{e}")
                summary_text = "ï¼ˆå¯¹è¯ä¸­æ®µæ‘˜è¦ï¼šåŒ…å«è‹¥å¹²æ­¥éª¤ã€é”™è¯¯ä¿®å¤ä¸ä¸­é—´ç»“è®ºï¼Œå·²çœç•¥ç»†èŠ‚ä»¥èŠ‚çœä¸Šä¸‹æ–‡ã€‚ï¼‰"

        # **å…³é”®ä¿®æ”¹ï¼šæŠŠâ€œå†å²æ€»ç»“â€ä½œä¸º user æ¶ˆæ¯å–‚ç»™æ¨¡å‹ï¼Œä»…ä½œä¸Šä¸‹æ–‡**
        summary_msg = {"role": "user", "content": f"[å†å²å¯¹è¯æ€»ç»“-ä»…ä¾›ä¸Šä¸‹æ–‡ï¼Œæ— éœ€å›å¤]\n{summary_text}"}

        new_history = ([sys_msg] if sys_msg else []) + [summary_msg] + tail
        new_history = model._validate_and_fix_tool_calls(new_history)

        # **å†æ¬¡ä¿è¯ system åç¬¬ä¸€æ¡æ˜¯ user**
        new_history = model._ensure_first_after_system_user(new_history)
        exact_total = tokens_of(new_history)

        if exact_total <= CONTEXT_TOKEN_HARD_LIMIT:
            msgs = sanitize_messages_for_openai(new_history)
            kwargs = {
                "api_key": model.api_key,
                "model": model.model,
                "messages": msgs,
                "stream": False,
                "request_timeout": REQUEST_TIMEOUT,
                "client_args": {"timeout": HTTPX_TIMEOUTS},
            }
            if model.base_url:
                kwargs["base_url"] = model.base_url
            resp = await acompletion(**kwargs)
            return resp.choices[0].message.content

        body = head + tail  # ä¸‹ä¸€è½®ç»§ç»­å‹ç¼©

    # å¤šè½®ä»è¶…é™ï¼šé€€è€Œæ±‚å…¶æ¬¡ â€”â€” ä»…ä¿ç•™ system + æçŸ­æ‘˜è¦ï¼ˆä»ä¸º userï¼‰
    try:
        minimal_summary = await summarize_chunk(body[:200])
    except Exception:
        minimal_summary = "ï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå·²å‹ç¼©ä¸ºæçŸ­æ‘˜è¦ã€‚ï¼‰"

    final_history = ([sys_msg] if sys_msg else []) + [
        {"role": "user", "content": f"[å†å²å¯¹è¯æç®€æ€»ç»“-ä»…ä¾›ä¸Šä¸‹æ–‡ï¼Œæ— éœ€å›å¤]\n{minimal_summary}"}
    ]
    final_history = model._ensure_first_after_system_user(final_history)
    msgs = sanitize_messages_for_openai(final_history)

    kwargs = {
        "api_key": model.api_key,
        "model": model.model,
        "messages": msgs,
        "stream": False,
        "request_timeout": REQUEST_TIMEOUT,
        "client_args": {"timeout": HTTPX_TIMEOUTS},
    }
    if model.base_url:
        kwargs["base_url"] = model.base_url
    resp = await acompletion(**kwargs)
    return resp.choices[0].message.content
